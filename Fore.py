import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
import warnings
import os

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')

from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l1_l2

# ==============================================================================
# FORECAST GRU (TARGETS ONLY) - Based on Haar Clusters
# ==============================================================================

# --- CONFIGURATION ---
INPUT_FILE = 'target_data_clustered.csv' # The file generated by the previous clustering script
WINDOW_SIZE = 45
FORECAST_HORIZON = 1
AUGMENTATION_FACTOR = 2
NOISE_LEVEL = 0.04
APPLY_SMOOTHING = True # Important to reduce noise captured by Wavelets
SMOOTHING_WINDOW = 3

# --- 1. LOAD DATA ---
print(f"Loading data from: {INPUT_FILE}")

if os.path.exists(INPUT_FILE):
    df = pd.read_csv(INPUT_FILE)
    
    # Normalization of names and types
    df.columns = [col.lower() for col in df.columns]
    df['created_at'] = pd.to_datetime(df['created_at'])
    df['date'] = df['created_at'].dt.date
    df['sku'] = df['sku'].astype(str)
    
    # Remove extra columns from previous merge if they exist
    if 'cluster_x' in df.columns: 
        df.rename(columns={'cluster_x': 'cluster'}, inplace=True)
    
    print(f"Data loaded: {len(df)} rows")
    print(f"   Clusters found: {sorted(df['cluster'].unique())}")
else:
    print("File not found. Run the Clustering script first.")
    exit()

# --- 2. PREPARATION AND CLEANING ---
# Daily Aggregation by Cluster
print("\nAggregating sales by day and cluster...")
daily_sales = df.groupby(['date', 'cluster'])['qty_ordered'].sum().reset_index()
daily_sales['date'] = pd.to_datetime(daily_sales['date'])

# Fill missing days in the middle of the series (if any)
idx = pd.date_range(daily_sales['date'].min(), daily_sales['date'].max())
daily_sales = daily_sales.set_index('date').groupby('cluster').apply(
    lambda x: x.reindex(idx, fill_value=0).drop(columns=['cluster'])
).reset_index()
daily_sales.rename(columns={'level_1': 'date'}, inplace=True)

# Smoothing (Smooth peaks for training)
if APPLY_SMOOTHING:
    daily_sales['qty_ordered'] = daily_sales.groupby('cluster')['qty_ordered'].transform(
        lambda x: x.ewm(span=SMOOTHING_WINDOW).mean()
    )

# --- 3. HELPER FUNCTIONS (Feature Engineering) ---
def add_features(df):
    df['day_of_week'] = df['date'].dt.dayofweek
    df['month'] = df['date'].dt.month
    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    # Lags and Rolling
    df['lag_7'] = df['qty_ordered'].shift(7).fillna(0)
    df['rolling_mean_7'] = df['qty_ordered'].rolling(7).mean().fillna(0)
    return df

def create_sequences(data, window_size):
    X, y = [], []
    for i in range(window_size, len(data)):
        X.append(data[i-window_size:i])
        y.append(data[i, 0]) # Target is the first column (qty)
    return np.array(X), np.array(y)

# --- 4. MODEL TRAINING (Loop by Cluster) ---
print("\nStarting GRU training by Cluster...")
results = {}
cluster_models = {}

for cluster_id in sorted(daily_sales['cluster'].unique()):
    print(f"\nCluster {cluster_id}")
    
    c_data = daily_sales[daily_sales['cluster'] == cluster_id].copy()
    c_data = add_features(c_data)
    
    # Select Features
    feature_cols = ['qty_ordered', 'day_sin', 'day_cos', 'month_sin', 'lag_7', 'rolling_mean_7']
    dataset = c_data[feature_cols].values
    
    # Scaling
    scaler = RobustScaler()
    dataset_scaled = scaler.fit_transform(dataset)
    
    # Split
    train_size = int(len(dataset_scaled) * 0.85)
    train_data, test_data = dataset_scaled[:train_size], dataset_scaled[train_size-WINDOW_SIZE:]
    
    X_train, y_train = create_sequences(train_data, WINDOW_SIZE)
    X_test, y_test = create_sequences(test_data, WINDOW_SIZE)
    
    if len(X_train) == 0: continue

    # Optimized GRU Model
    model = Sequential([
        GRU(64, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dropout(0.2),
        GRU(32, activation='tanh'),
        Dropout(0.2),
        Dense(1)
    ])
    
    model.compile(optimizer='adam', loss='huber')
    
    # Training with validation (CORRECTION HERE)
    model.fit(
        X_train, y_train, 
        validation_split=0.1,  # <--- THIS LINE WAS MISSING
        epochs=60, 
        batch_size=32, 
        verbose=0, 
        callbacks=[
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5) # Optional: Helps to converge
        ]
    )
    
    # Prediction
    pred = model.predict(X_test, verbose=0)
    
    # Inverse Scaling
    dummy = np.zeros((len(pred), len(feature_cols)))
    dummy[:, 0] = pred.flatten()
    pred_inv = scaler.inverse_transform(dummy)[:, 0]
    pred_inv = np.maximum(pred_inv, 0) # No negative sales
    
    dummy[:, 0] = y_test
    actual_inv = scaler.inverse_transform(dummy)[:, 0]
    
    # Metrics
    r2 = r2_score(actual_inv, pred_inv)
    print(f"   R2 Score: {r2:.3f}")
    
    results[cluster_id] = {
        'dates': c_data['date'].iloc[-len(pred_inv):].values,
        'actual': actual_inv,
        'pred': pred_inv
    }

# --- 5. TOP-DOWN DISTRIBUTION (Weights Based on Historical "Energy") ---
print("\nCalculating weights and distributing forecasts...")

# Calculate contribution weights of each SKU within its cluster
sku_weights = df.groupby(['cluster', 'sku'])['qty_ordered'].sum().reset_index()
cluster_totals = sku_weights.groupby('cluster')['qty_ordered'].transform('sum')
sku_weights['weight'] = sku_weights['qty_ordered'] / cluster_totals

final_forecasts = []

for cluster_id, res in results.items():
    # Get SKUs for this cluster
    cluster_skus = sku_weights[sku_weights['cluster'] == cluster_id]
    
    # For each predicted day
    for date, total_pred in zip(res['dates'], res['pred']):
        for _, row in cluster_skus.iterrows():
            final_forecasts.append({
                'date': date,
                'sku': row['sku'],
                'cluster': cluster_id,
                'qty_predicted': total_pred * row['weight']
            })

forecast_df = pd.DataFrame(final_forecasts)
forecast_df.to_csv('final_sku_forecasts.csv', index=False)

print("\nPerformance Visualization (Clusters):")
fig, axes = plt.subplots(len(results), 1, figsize=(12, 4*len(results)))
if len(results) == 1: axes = [axes]

for i, (cid, res) in enumerate(results.items()):
    axes[i].plot(res['dates'], res['actual'], label='Actual')
    axes[i].plot(res['dates'], res['pred'], label='GRU Forecast', linestyle='--')
    axes[i].set_title(f'Cluster {cid}')
    axes[i].legend()

plt.tight_layout()
plt.show()

print("\nDone. File generated: 'final_sku_forecasts.csv'")